<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>On the Edge of Stability</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,500;0,600;0,700;1,700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Hanken+Grotesk:wght@300;400;500;600;700;800;900&display=swap" rel="stylesheet">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="homepage">
        <!-- Nav spacer -->
        <div class="nav"></div>

        <!-- Hero Section -->
        <section class="hero">
            <div class="hero-content">
                <div class="hero-title-container">
                    <p class="hero-title">
                        <span class="title-sans">on the</span><br>
                        <span class="title-serif title-strikethrough">Edge</span><span class="title-sans"> of</span><br>
                        <span class="title-serif">stability.</span>
                    </p>
                    <div class="dot-grid-container" id="dotGridContainer">
                        <canvas id="dotCanvas"></canvas>
                    </div>
                </div>

                <div class="hero-text-wrapper">
                    <div class="hero-text">
                        <p class="hero-description">Exploring the delicate balance between sharpness and stability in deep learning, expanding on prior research by investigating how modern optimizers interact with the edge of stability, and providing insights into how these dynamics influence training outcomes and generalization in deep neural networks.</p>
                        <div class="button-group">
                            <button class="btn btn-primary">Report</button>
                            <button class="btn btn-secondary">Code</button>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Authors Section -->
        <section class="authors">
            <div class="author author-centered">
                <p class="author-name">Eric Cheng</p>
                <p class="author-email">e2cheng@ucsd.edu</p>
            </div>
            <div class="author author-centered">
                <p class="author-name">Matthew Jacobsen</p>
                <p class="author-email">majacobsen@ucsd.edu</p>
            </div>
            <div class="author author-centered">
                <p class="author-name">Prashant Potluri</p>
                <p class="author-email">prpotluri@ucsd.edu</p>
            </div>
            <div class="author author-centered">
                <p class="author-name">Zijin Qin</p>
                <p class="author-email">z2qin@ucsd.edu</p>
            </div>
            <div class="author author-centered">
                <p class="author-name">Tianhao Wang</p>
                <p class="author-email">tianhaowang@ucsd.edu</p>
            </div>
        </section>

        <!-- Sharpness & Stability Section -->
        <section class="content-section">
            <div class="section-two-column">
                <div class="section-title-vertical">
                    <p>Sharpness & </p>
                    <p>Stability</p>
                </div>
                <p class="section-text">In deep learning, <strong>sharpness</strong> refers to the curvature of the loss landscape at a specific point. Mathematically, it is the maximum eigenvalue of the Hessian matrix, the second order derivative, of the loss function. When training, a process called <strong>progressive sharpening</strong> occurs, in which the sharpness gradually increases. Progressive sharpening continues to occur until sharpness reaches a threshold of \(2 / \eta\), where \(\eta\) is the learning rate, where it then enters a regime called the <strong>edge of stability</strong>. This edge of stability is characterized by the stoppage of progressive sharpnening and sporadic oscillations in the still decreasing training loss.</div>
        </section>

        <!-- Quadratic Window -->
        <section class="window-box">
            <h2 class="window-title">Sharpness and Learning Rate</h2>
            <div class="window-content">
                <div class="placeholder-image">
                    <p>quadratic plot</p>
                </div>
                <p class="window-text">Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
            </div>
        </section>

        <!-- Gradient Descent Section -->
        <section class="content-section">
            <h2 class="section-title">Gradient Descent</h2>
            <div class="section-text-full">
                <p>Gradient Descent (GD) is the foundational optimization algorithm used to train deep neural networks. It operates by calculating the gradient of the loss function with respect to the model's parameters across the entire training dataset. The network's parameters are then updated by taking a step in the direction of the steepest descent, scaled by a fixed learning rate, \(\eta\). Mathematically, this deterministic update rule is defined as \(\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)\). Note that while full-batch Gradient Descent is often too computationally expensive for modern, large-scale datasets, it is often used as the theoretical bedrock for analyzing convergence and training dynamics, as shown below.</p>
                <p>&nbsp;</p>
                <p>In the context of the Edge of Stability, full-batch GD exhibits fascinating and counterintuitive behavior. Classical convex optimization theory dictates that GD will only converge stably if the learning rate is strictly bounded by the curvature of the loss landscape, specifically when \(\eta < 2 / \lambda_{max}\), where \(\lambda_{max}\) represents the sharpness. However, during the progressive sharpening phase of neural network training, GD naturally drives the loss landscape's curvature upward until it perfectly collides with this theoretical limit. Rather than diverging catastrophically when hitting \(2 / \eta\), GD rides this boundary. The optimizer enters the Edge of Stability, bouncing back and forth across the walls of the local minima, resulting in the non-monotonic but ultimately downward-trending loss curves characteristic of modern deep learning.</p>
            </div>
            <div class="image-grid-3">
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
            </div>
        </section>

        <!-- SGD Section -->
        <section class="content-section">
            <h2 class="section-title">Stochastic Gradient Descent</h2>
            <div class="section-text-full">
                <p>Eu habitasse euismod hac natoque at ut eleifend posuere nam. Augue habitasse montes iaculis varius mattis montes dolor id venenatis sit. Mauris congue lorem iaculis; gravida magna faucibus. Sapien viverra cum massa penatibus diam eros maecenas congue ornare porta quisque. Auctor blandit vehicula sem felis rhoncus mauris aenean netus nibh justo. Natoque lectus pulvinar orci ultricies nisl mauris semper dictumst. Dui phasellus ac et habitant elit tincidunt montes lacus. Mus ipsum condimentum enim facilisis convallis at lorem netus.</p>
                <p>&nbsp;</p>
                <p>Mattis suscipit libero placerat egestas cursus quisque egestas maecenas. Facilisi facilisi euismod sem mauris eu ac lorem vivamus et posuere. Quam posuere porttitor nisi senectus pulvinar mollis tempor mauris massa nam. Condimentum fringilla pretium, nec lobortis vivamus mattis. Sollicitudin eleifend viverra et ridiculus vel egestas libero.</p>
            </div>
            <div class="image-grid-3">
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
            </div>

            <div class="window-box">
                <h3 class="window-title">SGD on Varying Batch Sizes</h3>
                <div class="window-content-vertical">
                    <div class="image-stack">
                        <div class="placeholder-image-small"></div>
                        <div class="placeholder-image-small"></div>
                    </div>
                    <div class="window-text-vertical">
                        <!-- <p>&nbsp;</p> -->
                        <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. </p>
                        <p>&nbsp;</p>
                        <p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Adam Section -->
        <section class="content-section">
            <h2 class="section-title">Adam</h2>
            <div class="section-text-full">
                <p>Eu habitasse euismod hac natoque at ut eleifend posuere nam. Augue habitasse montes iaculis varius mattis montes dolor id venenatis sit. Mauris congue lorem iaculis; gravida magna faucibus. Sapien viverra cum massa penatibus diam eros maecenas congue ornare porta quisque. Auctor blandit vehicula sem felis rhoncus mauris aenean netus nibh justo. Natoque lectus pulvinar orci ultricies nisl mauris semper dictumst. Dui phasellus ac et habitant elit tincidunt montes lacus. Mus ipsum condimentum enim facilisis convallis at lorem netus.</p>
                <p>&nbsp;</p>
                <p>Mattis suscipit libero placerat egestas cursus quisque egestas maecenas. Facilisi facilisi euismod sem mauris eu ac lorem vivamus et posuere. Quam posuere porttitor nisi senectus pulvinar mollis tempor mauris massa nam. Condimentum fringilla pretium, nec lobortis vivamus mattis. Sollicitudin eleifend viverra et ridiculus vel egestas libero.</p>
            </div>
            <div class="image-grid-3">
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
            </div>
        </section>

        <!-- Muon Section -->
        <section class="content-section">
            <h2 class="section-title">Muon</h2>
            <div class="section-text-full">
                <p>Eu habitasse euismod hac natoque at ut eleifend posuere nam. Augue habitasse montes iaculis varius mattis montes dolor id venenatis sit. Mauris congue lorem iaculis; gravida magna faucibus. Sapien viverra cum massa penatibus diam eros maecenas congue ornare porta quisque. Auctor blandit vehicula sem felis rhoncus mauris aenean netus nibh justo. Natoque lectus pulvinar orci ultricies nisl mauris semper dictumst. Dui phasellus ac et habitant elit tincidunt montes lacus. Mus ipsum condimentum enim facilisis convallis at lorem netus.</p>
                <p>&nbsp;</p>
                <p>Mattis suscipit libero placerat egestas cursus quisque egestas maecenas. Facilisi facilisi euismod sem mauris eu ac lorem vivamus et posuere. Quam posuere porttitor nisi senectus pulvinar mollis tempor mauris massa nam. Condimentum fringilla pretium, nec lobortis vivamus mattis. Sollicitudin eleifend viverra et ridiculus vel egestas libero.</p>
            </div>
            <div class="image-grid-3">
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
            </div>
        </section>

        <!-- Shampoo Section -->
        <section class="content-section">
            <h2 class="section-title">Shampoo</h2>
            <div class="section-text-full">
                <p>Eu habitasse euismod hac natoque at ut eleifend posuere nam. Augue habitasse montes iaculis varius mattis montes dolor id venenatis sit. Mauris congue lorem iaculis; gravida magna faucibus. Sapien viverra cum massa penatibus diam eros maecenas congue ornare porta quisque. Auctor blandit vehicula sem felis rhoncus mauris aenean netus nibh justo. Natoque lectus pulvinar orci ultricies nisl mauris semper dictumst. Dui phasellus ac et habitant elit tincidunt montes lacus. Mus ipsum condimentum enim facilisis convallis at lorem netus.</p>
                <p>&nbsp;</p>
                <p>Mattis suscipit libero placerat egestas cursus quisque egestas maecenas. Facilisi facilisi euismod sem mauris eu ac lorem vivamus et posuere. Quam posuere porttitor nisi senectus pulvinar mollis tempor mauris massa nam. Condimentum fringilla pretium, nec lobortis vivamus mattis. Sollicitudin eleifend viverra et ridiculus vel egestas libero.</p>
            </div>
            <div class="image-grid-3">
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
                <div class="placeholder-image-small"></div>
            </div>
        </section>

        <!-- Conclusion Section -->
        <section class="content-section">
            <h2 class="section-title section-title-center">Conclusion</h2>
            <div class="section-text-full">
                <p>Eu habitasse euismod hac natoque at ut eleifend posuere nam. Augue habitasse montes iaculis varius mattis montes dolor id venenatis sit. Mauris congue lorem iaculis; gravida magna faucibus. Sapien viverra cum massa penatibus diam eros maecenas congue ornare porta quisque. Auctor blandit vehicula sem felis rhoncus mauris aenean netus nibh justo. Natoque lectus pulvinar orci ultricies nisl mauris semper dictumst. Dui phasellus ac et habitant elit tincidunt montes lacus. Mus ipsum condimentum enim facilisis convallis at lorem netus.</p>
                <p>&nbsp;</p>
                <p>Mattis suscipit libero placerat egestas cursus quisque egestas maecenas. Facilisi facilisi euismod sem mauris eu ac lorem vivamus et posuere. Quam posuere porttitor nisi senectus pulvinar mollis tempor mauris massa nam. Condimentum fringilla pretium, nec lobortis vivamus mattis. Sollicitudin eleifend viverra et ridiculus vel egestas libero.</p>
            </div>
        </section>


    </div>
    <!-- Footer -->
    <footer class="footer">
        <p>2026 Eric Cheng, Matthew Jacobsen, Prashant Potluri, Zijin Qin @ UC San Diego</p>
    </footer>

    <script src="scripts/dotgrid.js"></script>
</body>
</html>